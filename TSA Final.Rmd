---
title: "TSA Final Exam"
author: "Mary Esther Nevener"
date: "May 7, 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(bookdown)
```

## TIME SERIES ANALYSIS FINAL EXAM
###PROBLEM 1
1. [20pts] Consider the MA model $Z_T=0.5+\epsilon_t+\epsilon_{t-1}+\epsilon_{t-2}$
a.  Compute the theoretical ACF and plot it
b.  Generate $Z_t$ for $1\leq t \leq 1000$ and plot $Z_t$
c.  Compute the empirical ACF using the samples and compare it with your answer from (a)

####1a) 
$$
Z_T=0.5+\epsilon_t+\epsilon_{t-1}+\epsilon_{t-2}\\
\mu=0.5, \theta_1=1,  \theta_2 =1\\
\gamma_0=(1+\theta_1^2+\theta_2^2) \sigma^2 = 3 \sigma^2\\
\gamma_1=  (\theta_1+\theta_2\theta_1)\sigma^2 =2\sigma^2\\
\gamma_2=\theta_2\sigma^2 =\sigma_2\\
\rho_1=\frac{\theta_1 + \theta_1\theta_2}{1+\theta_1^2+\theta_2^2}=\frac{2}{3}\\
\rho_2=\frac{\theta_2}{1+\theta_1^2+\theta_2^2}=\frac{1}{3}\\
$$

```{r}
acfma2=ARMAacf(ma=c(1,1), lag.max=10)
acfma2
lags=0:10
plot(lags,acfma2,xlim=c(1,10), ylab="r",type="h", main = "Theoretical ACF for MA(2) with theta1 = 1,theta2=1")
abline(h=0)
``` 


####1b) 
```{r}
zc=arima.sim(n=1000, list(ma=c(1, 1)))
zt=zc+.5
plot(zt, type="b", main = "Simulated MA(2) Series")
``` 
 
####1c) 

##### There are two statistically significant spikes at lags 1 and 2 followed by non-significant values for other lags. However, due to sampling error, the sample ACF did not match the theoretical pattern exactly.

```{r}
acf(zt, xlim=c(1,10), main="ACF for simulated MA(2) Data")
``` 

###PROBLEM 3
3. [20pts] Derive an expression for the power spectral density,$s_y(\omega)$ for the ARMA(1,1) model and plot $s_y(\omega)$ for $0 \leq \omega \leq \pi$

$$
\textbf{POWER SPECTRAL DENSITY ARMA(1,1)}
$$

$$
\begin{align*}
\Phi(L)(y_t-\mu)&=\Psi(L)\epsilon_t\\
\Phi(L)&=1-\phi L\\
\Psi(L)&=1+\theta L\\
\therefore (y_t-m)&=\frac{\Psi(L)}{\Phi(L)}\epsilon_t=\bar{\psi}(L)\epsilon_t\\
g_y(z)&=\sigma^2\bar{\psi}(z)\bar{\psi}(z^{-1})\\
s_y(\omega)&=\frac{\sigma^2}{2\pi}\bar{\psi}(e^{-i\omega})\bar{\psi}(e^{i\omega})\\
\text{after factoring}\\
\Phi(z)&=1-\alpha z\\
\Psi(z)&=1+\beta z\\
\text{after substituting}\\
s_y(\omega)&=\frac{\sigma^2}{2\pi}\frac{1+\theta^2-2\theta \text{ cos}( \omega)}{1+\phi^2-2\phi \text{ cos}( \omega)}\\
\text{after normalizing}\\
\frac{2\pi}{\sigma^2}s_y(\omega)&=\frac{1+\theta^2-2\theta \text{ cos}( \omega)}{1+\phi^2-2\phi \text{ cos}( \omega)}\\
\end{align*}
$$
$$
\textbf{GRAPHING}\\
0<|\theta|<1\\
0<|\phi|<1\\
\begin{matrix} 
\omega=0 & \to & \frac{2\pi}{\sigma^2}s_y(\omega)=\frac{1+\theta^2-2\theta }{1+\phi^2-2\phi } & \to  & \frac{(\theta-1)^2}{(\phi-1)^2}\\
\omega=\frac{\pi}{2} & \to & \frac{2\pi}{\sigma^2}s_y(\omega)=\frac{1+\theta^2 }{1+\phi^2}  &\to & \frac{\theta^2+1}{\phi^2+1}\\
\omega=\pi &  \to & \frac{2\pi}{\sigma^2}s_y(\omega)=\frac{1+\theta^2+2\theta }{1+\phi^2+2\phi } &\to  & \frac{(\theta+1)^2}{(\phi+1)^2}\\
\end{matrix}
$$

```{r}
w<-c(0,pi/4,pi/2,3*pi/4,pi)
y<- function (phi,theta,w){(1+theta^2-2*theta*cos(w))/(1+phi^2-2*phi*cos(w))}

### WHEN THETA > PHI AND BOTH ARE POSITIVE
theta<-.55
phi<-  .45
plot (y(phi,theta,w),type="b", main="WHEN THETA > PHI AND BOTH ARE POSITIVE", xlab="OMEGA")

### WHEN THETA < PHI AND BOTH ARE POSITIVE
theta<-.45
phi<-  .55
plot (y(phi,theta,w),type="b", main="WHEN THETA < PHI AND BOTH ARE POSITIVE", xlab="OMEGA")

### WHEN THETA > PHI AND BOTH ARE NEGATIVE
theta<--.45
phi<-  -.55
plot (y(phi,theta,w),type="b", main="WHEN THETA > PHI AND BOTH ARE NEGATIVE", xlab="OMEGA")

### WHEN THETA < PHI AND BOTH ARE NEGATIVE
theta<--.55
phi<-  -.45
plot (y(phi,theta,w),type="b", main="WHEN THETA < PHI AND BOTH ARE NEGATIVE", xlab="OMEGA")

```

###PROBLEM 4
4. [20pts] Prove that conditional expectation minimizes the mean square error.

**PROOF:**

>Let our estimate $\hat{x}$ be a function of our known $y$ such that $\hat{x}=g(y)$. The error of our estimate is given by $\widetilde{X}=X-\hat{x}=X-g(y)$. The mean square error of our estimate is given by $E[(X-\bar{x})^2|Y=y]=E[(X-g(y))^2|Y=y]$. By using calculus we can calculate the minimun MSE.
$$
\begin{align*}
\text{MSE} &=E[(X-\hat{x})^2|Y=y]\\
&= E[X^2|Y=y]-2\hat{x}E[X|Y=y]+\hat{x^2}\\
\text{MSE'} &= -2E[X|Y=y]+2\hat{x}=0\\
\hat{x}_{\text{MIN}} &= E[X|Y=y]
\end{align*}
$$
Suppose that we would like to estimate the value of an unobserved random variable $X$, by observing the value of a random variable $Y=y$. In general, our estimate $\hat{x}$ is a function of $y$, so $\hat{X}=g(Y)$. Since $Y$ is a random variable, $\hat{X}=g(Y)$ is also a random variable. The error of our estimate is given by $\tilde{X}=X-\bar{X}=X-g(Y)$. The mean square error of our estimate is given by $E[(X-\hat{X})^2]=E[(X-g(Y))^2]$. By using calculus we can calculate the minimun MSE.
$$
\begin{align*}
\text{MSE} &=E[(X-\hat{X})^2|Y]\\
&= E[X^2|Y]-2\hat{X}E[X|Y]+\hat{X^2}\\
\text{MSE'} &= -2E[X|Y]+2\hat{X}=0\\
\hat{X}_{\text{MIN}} &= E[X|Y]
\end{align*}
$$

**END PROOF.**

###PROBLEM 5
5. [20pts] Compute expressions for the partial autocorrelation function for AR(1) and MA(1) models.

$$
\textbf{AR(1)}\\
x_t=\phi x_{t-1}+\epsilon_{t}\\
|\phi|\lt 1\\
\text{var}(\epsilon_t)=\sigma^2\\
\text{var}(x_t)=\frac{\sigma^2}{1-\phi^2}\\
$$

$$
\textbf{PACF FOR AR(1)}\\
\begin{align*}
\phi_{11}&=\frac{cov(x_t,x_{t+1})}{var(x_t)}\\
&=\frac{E[\phi x_{t-1}+\epsilon_t)x_{t-1}]}{var(x_t)}\\
&= \phi \frac{var(x_t)}{var(x_t)}\\
&= \phi\\
cov(e_t,e_{t+2})&= E[(x_t-\phi x_{t+1})(x_{t+2}-\phi x_{t+1})]\\
&= E[(x_t-\phi x_{t+1})\epsilon_{t+2}]\\
&=0\\
\phi_{22}&=\frac{cov(e_t,e_{t+2})}{[var(e_t)var(e_{t+2})]^\frac{1}{2}}\\
&= 0\\
\phi_{kk}&=0 \text{ for all k } \geq 2
\end{align*}
$$


$$
\textbf{MA(1)}\\
x_t=\epsilon_t+\theta\epsilon_{t-1}\\
var(x_t)=(1+\theta^2)\sigma^2\\
E[x_t.x_{t+1}]=\theta\sigma^2\\
E[x_t.x_{t+2}]=0\\
$$


$$
\textbf{PACF FOR MA(1)}\\
\begin{align*}
\phi_{11}&=\frac{E[x_t,x_{t+1}]}{var(x_t)}\\
&=\frac{\theta}{1+\theta^2}\\
\text{E}[e_t,e_{t+2}]&=\text{E}\big[ (x_t-\alpha x_{t+1})(x_{t+2}-\alpha x_{t+1})\big]\\
&=\text{E}[x_tx_{t+2}]-\alpha \text{E}[x_tx_{t+1}] - \alpha \text{E}[x_{t+1}x_{t+2}] -\alpha^2 \text{E}[x_{t+1}^2] \\
&= 0 -\alpha\theta\sigma^2  -\alpha\theta\sigma^2 + \alpha^2(1+\theta^2)\sigma^2\\
&=-\frac{2\theta^2}{(1+\theta^2)}\sigma^2+\frac{\theta^2\sigma^2}{(1+\theta^2)}\\
&=-\frac{\theta^2\sigma^2}{1+\theta^2}\\
\text{var}(e_t)&=\text{E}[x_t-\alpha x_{t+1}]^2\\
&= \text{E}(x_t^2)-2\alpha\text{E}(x_t x_{t+1})+ \alpha^2 \text{E}(x_{t+1}^2)\\
&= (1+\theta^2)\sigma^2-2\alpha \theta\sigma^2 + \alpha^2(1+\theta^2)\sigma^2\\
&= \frac{\sigma^2}{(1+\theta^2)}\big[1+\theta^2+\theta^4\big] \\
\text{var}(e_{t+2})&=\text{E}[x_{t+2}-\alpha x_{t+1}]^2\\
&= \text{E}(x_{t+2}^2)-2\alpha\text{E}(x_{t+2} x_{t+1})+ \alpha^2 \text{E}(x_{t+1}^2)\\
&= (1+\theta^2)\sigma^2-2\alpha \theta\sigma^2 + \alpha^2(1+\theta^2)\sigma^2\\
&= \frac{\sigma^2}{(1+\theta^2)}\big[1+\theta^2+\theta^4\big] \\
\phi_{22}&=\frac{E[e_te_{t+2}]}{\big[\text{var}(e_t)\text{var}(e_{t+2})\big]^{\frac{1}{2}}}\\
&=-\frac{\theta^2}{(1+\theta^2+\theta^4)} \\
\phi_{kk}&=\frac{(-1)^{k-1}\theta^k(1-\theta^2)}{1-\theta^{2(k+1)}}\\
\end{align*}
$$













###PROBLEM 2
2. [20pts] Download 1 time series containing trend and seasonality from http://www.statsci.org/datasets.html
a. Plot each series and compute its ACF and PACF
b. Based on the ACF and PACF, pick 2 distinct models and estimate their parameters
c. Compare the residual for each model
d. Which model is better and why?


####2a)  

```{r}
###PLOT OF NEW ZEALAND EMPLOYMENT NUMBERS FROM 1990 TO 2012
plot(nz)
  ###REMOVING TREND WITH LEAST SQUARES METHOD
    #CALCULATING A* and B*
    y<-nz$`Total Employed`
    t<-1:88
    ybar<-mean(y)
    tbar<-mean(t)
    bstar<-cov(y,t)/var(t)
    astar<- ybar-bstar*tbar
    #REMOVING TREND FROM Y TO GIVE US X
    xt<-y[t]-(astar+bstar*t)
    plot(xt)
  ###REMOVING SEASONALITY WITH METHOD OF LOCAL TREND
    n=88
    k=22
    d=4
    
    #TURNING DATA INTO MATRIX
    ymatrix<-matrix(xt,nrow=k, ncol=d)
    
    #FINDING AVERAGE OF EVERY ROW
    r1<-mean(ymatrix[1,])
    r2<-mean(ymatrix[2,])  
    r3<-mean(ymatrix[3,])
    r4<-mean(ymatrix[4,])    
    r5<-mean(ymatrix[5,])    
    r6<-mean(ymatrix[6,])    
    r7<-mean(ymatrix[7,])    
    r8<-mean(ymatrix[8,])    
    r9<-mean(ymatrix[9,])    
    r10<-mean(ymatrix[10,])    
    r11<-mean(ymatrix[11,])    
    r12<-mean(ymatrix[12,])    
    r13<-mean(ymatrix[13,])
    r14<-mean(ymatrix[14,])
    r15<-mean(ymatrix[15,])
    r16<-mean(ymatrix[16,])    
    r17<-mean(ymatrix[17,])
    r18<-mean(ymatrix[18,])
    r19<-mean(ymatrix[19,])
    r20<-mean(ymatrix[20,])
    r21<-mean(ymatrix[21,])
    r22<-mean(ymatrix[22,])    
    muhat<-c(r1,r2,r3,r4,r5,r6,r7,r8,r9,r10,r11,r12,r13,r14,r15,r16,r17,r18,r19,r20,r21,r22)
    
    #MAKING THE ROW AVERAGES A MATRIX
    muhatmatrix<-matrix(muhat,nrow=22,ncol=1)
  
    #SUBTRACTING THE AVERAGE OF EACH ROW FROM EVERY ELEMENT IN ROW
    ymatrix[,1]=ymatrix[,1]-muhatmatrix
    ymatrix[,2]=ymatrix[,2]-muhatmatrix
    ymatrix[,3]=ymatrix[,3]-muhatmatrix
    ymatrix[,4]=ymatrix[,4]-muhatmatrix
    
    #FINDING AVERAGE OF EVERY COLUMN OF NEW MATRIX
    c1<-mean(ymatrix[,1])   
    c2<-mean(ymatrix[,2])   
    c3<-mean(ymatrix[,3])   
    c4<-mean(ymatrix[,4])   
    shat<-c(c1,c2,c3,c4)
    
    #MAKING THE COLUMN AVERAGES A MATRIX
    shatmatrix<-matrix(shat,nrow=1,ncol=4)
    
    #SUBTRACTING THE AVERAGE OF EACH COLUMN FROM EVERY ELEMENT IN COLUMN
    ymatrix[1,]=ymatrix[1,]-shatmatrix
    ymatrix[2,]=ymatrix[2,]-shatmatrix
    ymatrix[3,]=ymatrix[3,]-shatmatrix
    ymatrix[4,]=ymatrix[4,]-shatmatrix
    ymatrix[5,]=ymatrix[5,]-shatmatrix
    ymatrix[6,]=ymatrix[6,]-shatmatrix
    ymatrix[7,]=ymatrix[7,]-shatmatrix
    ymatrix[8,]=ymatrix[8,]-shatmatrix
    ymatrix[9,]=ymatrix[9,]-shatmatrix
    ymatrix[10,]=ymatrix[10,]-shatmatrix
    ymatrix[11,]=ymatrix[11,]-shatmatrix
    ymatrix[12,]=ymatrix[12,]-shatmatrix
    ymatrix[13,]=ymatrix[13,]-shatmatrix
    ymatrix[14,]=ymatrix[14,]-shatmatrix
    ymatrix[15,]=ymatrix[15,]-shatmatrix     
    ymatrix[16,]=ymatrix[16,]-shatmatrix
    ymatrix[17,]=ymatrix[17,]-shatmatrix
    ymatrix[18,]=ymatrix[18,]-shatmatrix
    ymatrix[19,]=ymatrix[19,]-shatmatrix
    ymatrix[20,]=ymatrix[20,]-shatmatrix
    ymatrix[21,]=ymatrix[21,]-shatmatrix
    ymatrix[22,]=ymatrix[22,]-shatmatrix
    
    #TURNING THE NEW MATRIX BACK INTO DATA
    xt<-c(ymatrix[1,],ymatrix[2,],ymatrix[3,],ymatrix[4,],ymatrix[5,],ymatrix[6,],ymatrix[7,],ymatrix[8,],ymatrix[9,],ymatrix[10,],ymatrix[11,],ymatrix[12,],ymatrix[13,],ymatrix[14,],ymatrix[15,],ymatrix[16,],ymatrix[17,],ymatrix[18,],ymatrix[19,],ymatrix[20,],ymatrix[21,],ymatrix[22,])
    
###PLOTTING DETRENDED DATA
plot(xt, ylab="NZ EMPLOYMENT", xlab="TIME", type="b")

###COMPUTING ACF AND PACF
acfxt<-acf(xt, main="ACF FOR XT")
pacfxt<-pacf(xt, main="PACF FOR XT")
```
####2b)  




